import sys
from unit import Unit
#from enum import Enum


#TODO - break all token checks into individual functions, that can be easily called.
#       this would allow for a scientific notation token that checks for a number afterwords



class Scanner:

    #names for the token types
    #change these to all caps because they're constants
    comment = 'comment'
    whitespace = 'whitespace'
    number = 'number'
    operation = 'operation'
    unit = 'unit'
    parenthesis = 'parenthesis'
    #bracket = 'bracket'
    #identifier = 'identifier'
    EOF = 'EOF'

    
    def __init__(self, text):#filename):
        #self.text = open(filename, 'r').read() + '\a'   # ASCII bell so that it is guaranteed last token
                                                         # i.e. clearly the end of file if \a comes up
        self.text = text + '\a' * 10
        self.tokens = []

        self.tokanized = False

        
    def __str__(self):
        if not self.tokanized:
            return 'RAW INPUT:\n"' + self.text + '"'
        else:
            string = ''
            for t in self.tokens:
                string += t.type + ' ' * (14 - len(t.type))  + str(t.value) + '\n'
            return string
        
    
    def __repr__(self):
        string = ''
        if len(self.tokens) > 0:
            string += 'TOKANIZED INPUT:\n' + str(self.tokens)
            if len(self.text) > 0:
                string += '\n\n'

        if len(self.text) > 0:
            string += 'RAW INPUT:\n' + self.text
            
        return string

    def scan(self):
        self.scan0()    #convert raw text to only tokens
        #self.scan1()
        #self.scan2()
        #etc.

        #combine must be done in the parser
        #self.combine_units()
        
        # potentially skip this because whitespace might be important for some parsing decisions
        # i.e. are two pieces of code separated enough? For now, assume we aren't checking this
        # might need to insert multiply op in place of some spaces? can the parser figure it out?
        self.remove_whitespace()

        self.tokanized = True
        
        return self.tokens


    def remove_whitespace(self):
        tokens = []
        for t in self.tokens:
            if t.type != Scanner.whitespace:
                tokens.append(t)

        self.tokens = tokens

            
                
    
    def scan0(self):
        """Convert raw text to a series of tokens. No optimizations/combinations are made"""
        passes = 0
        
        while len(self.text) > 1:    # loop until all input is consumed
            passes += 1
            ate_tokens = False

            
            ##### MAKE SURE THIS IS THE FIRST TOKEN TO BE CHECKED FOR #####
            #EOF token -> array ends with the ASCII bell symbol
            if self.text[0] == '\a':    #ascii bell
                self.text = ''
                break


            if self.eat_single_comment():
                ate_tokens = True
                continue

            if self.eat_multi_comment():
                ate_tokens = True
                continue

            if self.eat_all_whitespace():
                ate_tokens = True
                continue
            
            if self.eat_hex_number():
                ate_tokens = True
                continue

            if self.eat_bin_number():
                ate_tokens = True
                continue

            if self.eat_number():
                ate_tokens = True
                continue


            if self.eat_unit():
                ate_tokens = True
                continue

            if self.eat_parenthesis():
                ate_tokens = True
                continue


            if not ate_tokens:
                print('no recognized tokens on pass ' + str(passes) + '. exiting scanner')
                print(self)
                return #potentially return the tokan_list, or maybe something to signify incomplete

            

            
            
            continue
            #skip all of the stuff below this


            
            #### all other tokanizations should be sorted from longest to shortest (in terms of the first character looked at) so that if a shorter token might match, the longer one will get it first before the short match messes up the string

            #Identifiers should be last?
            #move from most specific to most generic types of tokens
            
            
            #comment tokens (single line)
            if self.text[:2] == '//':
                i=2
                while self.text[i] != '\n':
                    i+=1
                self.tokens.append(Token(Scanner.comment, self.text[:i]))    # add comment to token list 
                self.tokens.append(Token(Scanner.whitespace, '\n'))          # add newline at the end to token list
                self.text = self.text[i+1:]                          # update string to exclude parsed token
                ate_tokens = True
                continue

            #comment tokens (multiline)
            if self.text[:2] == '/{':
                i=2
                while self.text[i:i+2] != '}/':
                    i+=1
                self.tokens.append(Token(Scanner.comment, self.text[:i+1]))
                self.text = self.text[i+2:]
                ate_tokens = True
                continue

                
            #whitespace tokens
            if self.text[0] == '\n':    #newline
                i=1
                while self.text[i] == '\n':
                    i+=1
                self.tokens.append(Token(Scanner.whitespace, ('newline', i)))
                self.text = self.text[i:]
                ate_tokens = True
                continue
                
            if self.text[0] == ' ':     #space
                i=1
                while self.text[i] == ' ':
                    i+=1
                self.tokens.append(Token(Scanner.whitespace, ('space', i)))
                self.text = self.text[i:]
                ate_tokens = True
                continue
                
            if self.text[0] == '\t':    #tab
                i=1
                while self.text[1] == '\t':
                    i+=1
                self.tokens.append(Token(Scanner.whitespace, ('tab', i)))
                self.text = self.text[i:]
                ate_tokens = True
                continue
                
            if self.text[0] == '\r':    #carriage return
                i=1
                while self.text[i] == '\r':
                    i+=1
                self.tokens.append(Token(Scanner.whitespace, ('carriage return', i)))
                self.text = self.text[i:]
                ate_tokens = True
                continue
                
            #'\b' backspace

            #'\f' formfeed

            #'\v' vertical tab
                

            #number prefix
            #e.g. 0x 0b
            
            #number token
            if self.text[0].isnumeric():
                i=1
                while self.text[i].isnumeric():
                    i+=1
                if self.text[i] == ".":
                    i+=1
                    while self.text[i].isnumeric():
                        i+=1
                self.tokens.append(Token(Scanner.number, self.text[:i]))
                self.text = self.text[i:]
                ate_tokens = True
                continue

            #scientific notation
            #if self.text[0] in 'Ee':
            #    if self.text[1] == '-':
            #        #check for number
            #        pass
            #    else:
            #        #check for number without
            #        pass

            #operation token
            if self.text[0] in '+-*/%^':
                self.tokens.append(Token(Scanner.operation, self.text[0]))
                self.text = self.text[1:]
                ate_tokens = True
                continue

            
            #SI unit token
            check = Unit.match_units(self.text)
            if check is not None:
                (p, u) = check
                self.text = self.text[len(p)+len(u):] #eat text
                
                if p == '': #set prefix to none if empty
                    p = None
                self.tokens.append(Token(Scanner.unit, (p, u)))
                ate_tokens = True
                continue
                
                
                            
            #other tokens
            #e.g. identifier.
            

            #parenthesis tokens
            if self.text[0] in '()':
                self.tokens.append(Token(Scanner.parenthesis, self.text[0]))
                self.text = self.text[1:]
                ate_tokens = True
                continue

            
            if not ate_tokens:
                print('no recognized tokens on pass ' + str(passes) + '. exiting scanner')
                print(self)
                return #potentially return the tokan_list, or maybe something to signify incomplete

            
        #end of scan0 loop
        pass


    #whitespace tokens
    def eat_all_whitespace(self):    #eat all possible whitespace tokens
        ate_tokens = False
        while self.eat_newline() or self.eat_space() or self.eat_tab() or self.eat_carriage_return():
            ate_tokens = True
        return ate_tokens
        
    
    def eat_newline(self):
        if self.text[0] == '\n':    #newline
            i=1
            while self.text[i] == '\n':
                i+=1
            self.tokens.append(Token(Scanner.whitespace, ('newline', i)))
            self.text = self.text[i:]
            return True
        return False

    
    def eat_space(self):
        if self.text[0] == ' ':     #space
            i=1
            while self.text[i] == ' ':
                i+=1
            self.tokens.append(Token(Scanner.whitespace, ('space', i)))
            self.text = self.text[i:]
            return True
        return False
    

    def eat_tab(self):
        if self.text[0] == '\t':    #tab
            i=1
            while self.text[1] == '\t':
                i+=1
            self.tokens.append(Token(Scanner.whitespace, ('tab', i)))
            self.text = self.text[i:]
            return True
        return False

    def eat_carriage_return(self):
        if self.text[0] == '\r':    #carriage return
            i=1
            while self.text[i] == '\r':
                i+=1
            self.tokens.append(Token(Scanner.whitespace, ('carriage return', i)))
            self.text = self.text[i:]
            return True
        return False
            


    def eat_single_comment(self):
        if self.text[:2] == '//':    #comment tokens (single line)
            i=2
            while self.text[i] != '\n':
                i+=1
            self.tokens.append(Token(Scanner.comment, self.text[:i]))    # add comment to token list 
            self.tokens.append(Token(Scanner.whitespace, '\n'))          # add newline at the end to token list
            self.text = self.text[i+1:]                          # update string to exclude parsed token
            return True
        return False

    def eat_multi_comment(self):
        if self.text[:2] == '/{':    #comment tokens (multiline)
            i=2
            while self.text[i:i+2] != '}/':
                i+=1
            self.tokens.append(Token(Scanner.comment, self.text[:i+1]))
            self.text = self.text[i+2:]
            return True
        return False

        
    def eat_hex_number(self):
        if self.text[:2] == '0x':    #eat a hex number, e.g. 0x13DF45FC
            i=2
            while self.text[i] in "0123456789abcdefABCDEF":
                i+=1

            if i > 2:
                self.tokens.append(Token(Scanner.number, self.text[:i]))
                self.text = self.text[i+1:]
                return True

        return False
            

    def eat_bin_number(self):
        if self.text[:2] == '0b':    #eat a binary number e.g. 0b110011101
            i=2
            while self.text[i] in "01":
                i+=1

            if i > 2:
                self.tokens.append(Token(Scanner.number, self.text[:i]))
                self.text = self.text[i+1:]
                return True

        return False
    

    def eat_number(self):
        #number token
        if self.text[0].isnumeric():
            i=1
            while self.text[i].isnumeric():
                i+=1
            if self.text[i] == ".":
                i+=1
                while self.text[i].isnumeric():
                    i+=1
            self.tokens.append(Token(Scanner.number, self.text[:i]))
            self.text = self.text[i:]
            return True
        return False

    def eat_sci_notation(self):
        #scientific notation
        #if self.text[0] in 'Ee':
        #    if self.text[1] == '-':
        #        #check for number
        #        pass
        #    else:
        #        #check for number without
        #        pass
        pass


    def eat_unit(self):
        check = Unit.match_units(self.text)  #SI unit token
        if check is not None:
            (p, u) = check
            self.text = self.text[len(p)+len(u):] #eat text
                
            if p == '': #set prefix to none if empty
                p = None
            self.tokens.append(Token(Scanner.unit, (p, u)))
            return True
        return False
                
                
                            
    #other tokens
    #e.g. identifiers, boolean operations, etc.

    
    def eat_parenthesis(self):
        #parenthesis tokens
        if self.text[0] in '()':
            self.tokens.append(Token(Scanner.parenthesis, self.text[0]))
            self.text = self.text[1:]
            return True
        return False




    
class Token:
    """Class for collecting tokens from the input .dewy file in the Dewy compiler process chain."""

    def __init__(self, type, value):
        #do something to initialize the token
        self.type = type
        self.value = value
        

    def __str__(self):
        #convert the token to a string representation
        return 'token({}, {})'.format(self.type, repr(self.value))
        

    def __repr__(self):
        #return a representation of the token
        return self.__str__()

    


if __name__ == "__main__":
    
    if len(sys.argv) > 1: #input file mode

        filename = sys.argv[1]
        text = open(filename, 'r').read()
        s = Scanner(text)
        s.scan()
        print(str(s))
        
    else:  #interpretor mode

        while True:
            string = input(">>> ")
            s = Scanner(string)
            s.scan()
            print(str(s))
